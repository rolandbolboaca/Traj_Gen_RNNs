{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Info**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***01.04.2025 by Roland Bolboaca***\n",
    "\n",
    "0. **Coordinate System**\n",
    "    - Decimal Degree (DD) - https://gisgeography.com/latitude-longitude-coordinates/\n",
    "                          - https://www.ncesc.com/geographic-faq/what-are-the-different-gps-coordinate-formats/ \n",
    "                          - https://en.wikipedia.org/wiki/Geographic_coordinate_system\n",
    "                          - https://www.luxwisp.com/types-of-gps-coordinates-explained/\n",
    "                          - https://www.hec.usace.army.mil/confluence/cwmsdocs/cwmsum/3.4/setting-up-the-coordinate-system/coordinate-system-types\n",
    "    - Converters: https://www.fcc.gov/media/radio/dms-decimal\n",
    "1. **Dataset link:** \n",
    "\n",
    "    - Information about the first atasets, namely San Francisco(Caltrain) and Sao Bento(Porto).\n",
    "        - Link: https://github.com/bguillouet/trajectory_classification\n",
    "\n",
    "    - Download:\n",
    "\n",
    "        - San Francisco Dataset: https://ieee-dataport.org/open-access/crawdad-epflmobility \n",
    "\n",
    "        - Porto Dataset: https://archive.ics.uci.edu/dataset/339/taxi+service+trajectory+prediction+challenge+ecml+pkdd+2015 \n",
    "\n",
    "        - Multiple Datasets: https://dl.acm.org/doi/fullHtml/10.1145/3355283 (Table 3)\n",
    "\n",
    "2. **Teri Preprocessed Dataset:** \n",
    "    \n",
    "    - Pkl Files: \n",
    "        - datasets/porto_trajectories.pkl and porto_uci_31k_traj.pkl\n",
    "        - This dataset contains a list of pandas dataframes.\n",
    "        - Each datafarme contains a trajectory with 2 features (lat, lon) for now.\n",
    "\n",
    "3. **Directories:**\n",
    "\n",
    "    - Datasets: holds the datasets as pkl files.\n",
    "    - Pkls: holds the results as pkl files (S2S and P2P, SL, HU, COMP)\n",
    "    - Plots: holds the plots for the results.\n",
    "\n",
    "4. **Flow:**\n",
    "\n",
    "    - Create and activate venv: .\\venv\\Scripts\\activate  \n",
    "    - Check and install requirements.txt: pip install -r requirements.txt\n",
    "\n",
    "    - Set parameters and Hyperparameters below.\n",
    "    - Load Trajectory Data:\n",
    "        - Load from pkl.\n",
    "        - Select only trajectories from the defined square.\n",
    "        - Select only the needed number of trajectories.\n",
    "        - Get mins and maxs (for manual norm/denorm).\n",
    "        - Create X and Y from data.\n",
    "    - Create Train/Test splits:\n",
    "        - Train data preparation.\n",
    "        - Test data preparation.\n",
    "            - Can be performed with additional testing file or by splitting a single file into train/test.\n",
    "        - For each trajectory, the values are padded (end) with zeroes to a multiple of the SL (for data reshaping).\n",
    "        - After inference, the padded vcalues are removed.\n",
    "    - Testing flow:\n",
    "        - Create LSTM/RNN/GRU models\n",
    "        - Train models.\n",
    "        - Test Models.\n",
    "        - Compute model metrics: SE, IE, and MSE.\n",
    "        - Denormalize data.\n",
    "        - Plot results.\n",
    "\n",
    "5. **Included full experiments:**\n",
    "    \n",
    "    - **Dynamic Sequence Length (SL):**\n",
    "        - Train with a given SL, which defults to 25.\n",
    "        - Test with a batch size of 1 and a SL of 25.\n",
    "        - Test with a batch size of 1 and a SL of 1.\n",
    "            - Can run inference in real time even with a single point.\n",
    "        - Almost identical results for the above two experiments.\n",
    "        - Denormalize, compute metrics, plot.\n",
    "        - Save results.\n",
    "\n",
    "    - **SL performance analysis:**\n",
    "        - Train with SLs between 3 and 40 with an increasing step of 3.\n",
    "        - Test with the same SLs and a batch size of 1.\n",
    "        - Denormalize, compute metrics, plot.\n",
    "        - Save results.\n",
    "\n",
    "    - **Hidden Units (HU) performance analysis:**\n",
    "        - Train with various number of HU, ranging from 2 to 64.\n",
    "        - Test with the same number of HU.\n",
    "        - Denormalize, compute metrics, plot.\n",
    "        - Save results.\n",
    "\n",
    "    - **LSTM, GRU, RNN model performance analysis:**\n",
    "        - Train and test the performance of the three models.\n",
    "        - Denormalize, compute metrics, plot.\n",
    "        - Save results.\n",
    "\n",
    "    - **Keras training approach or manual training loop with sequence reset:**\n",
    "        - Train and test the performance of both models.\n",
    "        - Denormalize, compute metrics, plot.\n",
    "        - Analyse results.\n",
    "        \n",
    "\n",
    "Feel free to change and enhance the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Module Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "Imports"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 11:11:10.039488: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749715870.052990 2564041 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749715870.057245 2564041 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749715870.069155 2564041 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749715870.069174 2564041 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749715870.069175 2564041 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749715870.069176 2564041 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-12 11:11:10.072725: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/roland/traj_exp/venv/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "# oneDNN warning suppression TF 2.4.1\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import gc\n",
    "from ipywidgets import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import energy_distance, wasserstein_distance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from math import radians\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# SEED = 837\n",
    "\n",
    "SEED = 123456\n",
    "TESTING_SIZE = 0.2\n",
    "\n",
    "# Total number of trajectories. If set to 0, all trajectories are used\n",
    "TOTAL_TRAJS = 0\n",
    "\n",
    "TRAINING_TESTING_SAME_FILE = True\n",
    "\n",
    "SELECTED_DATASET = \"SANFRANCISCO\" \n",
    "# SELECTED_DATASET = \"PORTO\"  \n",
    "\n",
    "\n",
    "DATASET = {\"PORTO\": \"datasets/Porto/porto_uci_31k_traj_drop_only.pkl\",\n",
    "           \"SANFRANCISCO\": \"datasets/San_Francisco/train_trajectories.pkl\"}\n",
    "\n",
    "\n",
    "# DATASET = {\"PORTO\": \"datasets/Porto/porto_uci_31k_traj_time_diff.pkl\",\n",
    "#            \"SANFRANCISCO\": \"datasets/San_Francisco/train_trajectories_time_diff.pkl\"}\n",
    "\n",
    "TESTING_FILE = None\n",
    "\n",
    "COLUMNS = [\"lat\", \"lon\"]\n",
    "           \n",
    "# COLUMNS = [\"lat\", \"lon\", \"distance_km\", \"speed_km\"]\n",
    "\n",
    "COLUMNS_INPUT = [\"lat\", \"lon\"]\n",
    "COLUMNS_OUTPUT = [\"lat\", \"lon\"]\n",
    "\n",
    "# COLUMNS = [\"lat\", \"lon\", \"distance_km\"]\n",
    "\n",
    "# COLUMNS = [\"lat\", \"lon\", \"distance_km\", \"speed_km\"]\n",
    "\n",
    "\n",
    "\n",
    "DATA_SQUARE_SF = { \n",
    "                \"lat_1\": 37.86499,\n",
    "                \"lon_1\": -122.53304,\n",
    "                \"lat_2\": 37.68481,\n",
    "                \"lon_2\": -122.30576\n",
    "                }\n",
    "\n",
    "DATA_SQUARE_PORTO = { \n",
    "                \"lon_1\": 41.23969,\n",
    "                \"lat_1\": -8.73005,\n",
    "                \"lon_2\": 41.05951,\n",
    "                \"lat_2\": -8.49195\n",
    "                }\n",
    "\n",
    "DATA_SQUARE = {\"SANFRANCISCO\": DATA_SQUARE_SF,\n",
    "               \"PORTO\": DATA_SQUARE_PORTO}\n",
    "\n",
    "DATA_CENTER_PORTO = {\n",
    "    \"lat\": 41.14961, # Latitude of Porto\n",
    "    \"lon\": -8.61099 # Longitude of Porto\n",
    "}\n",
    "\n",
    "DATA_CENTER_SF = {\n",
    "    \"lat\": 37.7749,  # Latitude of San Francisco\n",
    "    \"lon\": -122.4194  # Longitude of San Francisco\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "LSTM_CELLS = 32 #32\n",
    "SEQ_LEN = 25 #25\n",
    "BATCH_SIZE = 32 #32\n",
    "EPOCHS = 100\n",
    "LR = 0.01\n",
    "\n",
    "STATEFUL = False\n",
    "RETURN_SEQ = True\n",
    "\n",
    "NUM_FEATS = 2\n",
    "NUM_OUTPUTS = 2\n",
    "\n",
    "FEATS = [0, 1, 2] # lat, lon, speed_km\n",
    "OUTPUTS = [0, 1] # lat, lon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(data, data_path):\n",
    "    \"\"\" Save data to a pickle file \"\"\"\n",
    "    \n",
    "    with open(data_path, 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(data_path):\n",
    "    \"\"\" Load data from file path\"\"\"\n",
    "    \n",
    "    with open(data_path, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_pickle(data_path, num_of_traj = 0):\n",
    "    \"\"\" Load data from pickle file \"\"\"\n",
    "    \n",
    "    df = load_pickle(data_path)\n",
    "    \n",
    "    if num_of_traj != 0:\n",
    "        num_of_traj = len(df)\n",
    "        df = df[:num_of_traj]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_Y_from_data(data, num_of_traj):\n",
    "    \"\"\"\n",
    "        Create X and Y from the data.\n",
    "        X is composed of the trajectory data starting from the first point to the second last point\n",
    "        Y is composed of the trajectory data starting from the second point to the last point\n",
    "        Keeps only the selected COLUMNS (defined in the parameters block)\n",
    "    \"\"\"\n",
    "    X, Y = [0.0]  * num_of_traj, [0.0] * num_of_traj\n",
    "\n",
    "\n",
    "    for i in range(num_of_traj):\n",
    "        # X is composed of the trajectory data starting from the first point to the second last point\n",
    "        X[i] =  data[i][COLUMNS_INPUT].iloc[0:-1] \n",
    "        X[i] = X[i].fillna(0)\n",
    "        X[i].columns = COLUMNS_INPUT\n",
    "\n",
    "        # Y is composed of the trajectory data starting from the second point to the last point\n",
    "        Y[i] =  data[i][COLUMNS_OUTPUT].iloc[1:] \n",
    "        Y[i] = Y[i].fillna(0)\n",
    "        Y[i].columns = COLUMNS_OUTPUT\n",
    "        \n",
    "        X[i] = X[i].to_numpy()\n",
    "        Y[i] = Y[i].to_numpy()\n",
    "        \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dummy_data():\n",
    "    \"\"\" Dummy Dataset Load for custom experiments\"\"\"\n",
    "    \n",
    "    uri = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00616/Tetuan%20City%20power%20consumption.csv\"\n",
    "    keras.utils.get_file(\"ds.csv\",\"https://archive.ics.uci.edu/ml/machine-learning-databases/00616/Tetuan%20City%20power%20consumption.csv\" )\n",
    "    filename = \"ds.csv\"\n",
    "    urlretrieve(uri, filename)\n",
    "\n",
    "    data_df = pd.read_csv(\"ds.csv\")\n",
    "    data_df[\"DateTime\"] = pd.to_datetime(data_df[\"DateTime\"])\n",
    "    data_df = data_df.set_index('DateTime')\n",
    "    data_df.columns = [col.strip() for col in data_df.columns]\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Data preprocessing, Normalization and Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_in_square(data, square):\n",
    "    \"\"\" Get data inside the square defined by the square dictionary \"\"\" \n",
    "    \n",
    "    # Ensure correct bounds regardless of coordinate sign or order\n",
    "    lat_min = min(square[\"lat_1\"], square[\"lat_2\"])\n",
    "    lat_max = max(square[\"lat_1\"], square[\"lat_2\"])\n",
    "    lon_min = min(square[\"lon_1\"], square[\"lon_2\"])\n",
    "    lon_max = max(square[\"lon_1\"], square[\"lon_2\"])\n",
    "\n",
    "    filtered_data = []\n",
    "    \n",
    "    for traj in data:\n",
    "        in_lat_bounds = traj[\"lat\"].between(lat_min, lat_max)\n",
    "        in_lon_bounds = traj[\"lon\"].between(lon_min, lon_max)\n",
    "\n",
    "        if (in_lat_bounds & in_lon_bounds).all():\n",
    "            filtered_data.append(traj)\n",
    "            \n",
    "            \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_max_from_data(data):\n",
    "    # Get trajectories min and max values\n",
    "    num_of_traj = len(data)\n",
    "    mins, maxs = [0.0] * num_of_traj, [0.0] * num_of_traj\n",
    "\n",
    "    for i in range(num_of_traj):\n",
    "        mins[i]  = np.array(data[i].min()) \n",
    "        maxs[i] = np.array(data[i].max()) \n",
    "\n",
    "    mins =  np.min( np.array(mins), axis = 0)[0 : 2]\n",
    "    maxs =  np.max( np.array(maxs), axis = 0)[0 : 2]\n",
    "    \n",
    "    return mins, maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(dataset, \n",
    "                   normalization_type = 'min-max',\n",
    "                   normalization_ranges = None,\n",
    "                   testing_data_norm = False):\n",
    "    \"\"\"\n",
    "        Function to normalize the dataset using either min-max or standard normalization.\n",
    "        Can be used for separate testing data normalization or for normalization of the whole dataset with other ranges.\n",
    "        Returns the normalized dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    if normalization_type == 'min-max':   \n",
    "        scaler = MinMaxScaler()\n",
    "    elif normalization_type == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    # Normalize the dataset using the ranges given in normalization_ranges (min and max)        \n",
    "    # Used for separate testing data normalization or for normalization of the whole dataset with other ranges\n",
    "    \n",
    "    if normalization_ranges is not None:\n",
    "        scaler.min = normalization_ranges[\"min\"]\n",
    "        scaler.max = normalization_ranges[\"max\"]\n",
    "    else:\n",
    "        scaler.fit(dataset)\n",
    "        \n",
    "    columns = dataset.columns\n",
    "    \n",
    "    norm_dataset = scaler.transform(dataset)\n",
    "    norm_dataset = pd.DataFrame(norm_dataset, columns = columns)\n",
    "    \n",
    "    return scaler, norm_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_trajectory_data(dataset, \n",
    "                   normalization_type = 'min-max',\n",
    "                   normalization_ranges = None,\n",
    "                   testing_data_norm = False,\n",
    "                   scaler = None):\n",
    "    \"\"\"\n",
    "        Function to normalize the dataset using either min-max or standard normalization.\n",
    "        Can be used for separate testing data normalization or for normalization of the whole dataset with other ranges.\n",
    "        Returns the normalized dataset.\n",
    "    \"\"\"\n",
    "    dataset_cpy = copy.deepcopy(dataset)\n",
    "    \n",
    "    if testing_data_norm is False:\n",
    "        if scaler is None:\n",
    "            if normalization_type == 'min-max':   \n",
    "                scaler = MinMaxScaler()\n",
    "            elif normalization_type == 'standard':\n",
    "                scaler = StandardScaler()\n",
    "        \n",
    "        # Normalize the dataset using the ranges given in normalization_ranges (min and max)        \n",
    "        # Used for separate testing data normalization or for normalization of the whole dataset with other ranges\n",
    "        \n",
    "        if normalization_ranges is not None:\n",
    "            X_min = normalization_ranges[\"min\"]\n",
    "            X_max = normalization_ranges[\"max\"]\n",
    "            dataset_cpy = [(arr - X_min) / (X_max - X_min) for arr in dataset_cpy]\n",
    "            \n",
    "        else:\n",
    "            dataset_flat = pd.concat(dataset_cpy, ignore_index=True)\n",
    "            # dataset_flat = np.vstack(dataset_cpy)\n",
    "            scaler.fit(dataset_flat)\n",
    "            \n",
    "            columns = dataset_cpy[0].columns\n",
    "            \n",
    "            for i in range(len(dataset_cpy)):\n",
    "                norm_dataset = scaler.transform(dataset_cpy[i])\n",
    "                norm_dataset = pd.DataFrame(norm_dataset, columns = columns)\n",
    "                dataset_cpy[i] = norm_dataset\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        if normalization_ranges is not None:\n",
    "            X_min = normalization_ranges[\"min\"]\n",
    "            X_max = normalization_ranges[\"max\"]\n",
    "            dataset_cpy = [(arr - X_min) / (X_max - X_min) for arr in dataset_cpy]\n",
    "            \n",
    "        else:\n",
    "            columns = dataset_cpy[0].columns\n",
    "        \n",
    "            for i in range(len(dataset_cpy)):\n",
    "                norm_dataset = scaler.transform(dataset_cpy[i])\n",
    "                norm_dataset = pd.DataFrame(norm_dataset, columns = columns)\n",
    "                dataset_cpy[i] = norm_dataset\n",
    "            \n",
    "    return scaler, dataset_cpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Denormalize Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_data(dataset, scaler = None, normalization_ranges = None):\n",
    "    \"\"\"\n",
    "        Function to denormalize the dataset using the scaler used to normalize the dataset.\n",
    "        Manual denormalization can be used for separate testing data denormalization or for denormalization of the whole dataset.\n",
    "    \"\"\"\n",
    "    dataset_cpy = copy.deepcopy(dataset)\n",
    "    \n",
    "    #######\n",
    "    if scaler is None and normalization_ranges is not None:\n",
    "        X_min = normalization_ranges[\"min\"]\n",
    "        X_max = normalization_ranges[\"max\"]\n",
    "        \n",
    "        dataset_cpy = [arr * (X_max - X_min) + X_min for arr in dataset]\n",
    "            \n",
    "    if scaler is not None:\n",
    "        for item in range(len(dataset)):\n",
    "            dataset_cpy[item] = scaler.inverse_transform(dataset_cpy[item])\n",
    "       \n",
    "    return dataset_cpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Data preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reshape(X, Y, \n",
    "                seq_len,\n",
    "                num_feats, \n",
    "                num_outputs,\n",
    "                batch_size):\n",
    "    \"\"\"\n",
    "        Function to split the data into training and testing sets and reshape the data into the required shape for the LSTM model\n",
    "        Returns a dictionary with the following:\n",
    "            X_train: Training data for the input features\n",
    "            X_test: Testing data for the input features\n",
    "            Y_train: Training data for the output features\n",
    "            Y_test: Testing data for the output features    \n",
    "    \"\"\"\n",
    "    \n",
    "    valid_rows = X.shape[0] // seq_len * seq_len\n",
    "\n",
    "    if len(Y.shape) == 1:\n",
    "        X = X[:valid_rows]\n",
    "        Y = Y[:valid_rows]\n",
    "    else:\n",
    "        X = X[:valid_rows, :]\n",
    "        Y = Y[:valid_rows, :]\n",
    "\n",
    "    num_sequences = X.shape[0] // seq_len\n",
    "       \n",
    "    X = X.reshape(num_sequences, seq_len, num_feats)\n",
    "    Y = Y.reshape(num_sequences, seq_len, num_outputs)\n",
    "              \n",
    "    X_train = X[0: X.shape[0] - (X.shape[0] % batch_size)]\n",
    "    Y_train = Y[0: Y.shape[0] - (Y.shape[0] % batch_size)]\n",
    "       \n",
    "    data = {\"X_train\": X_train, \"Y_train\": Y_train}\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raagged_data(X, Y, \n",
    "                        seq_len, \n",
    "                        batch_size,\n",
    "                        num_trajectories):\n",
    "        \"\"\"\n",
    "        Create a ragged dataset for the LSTM model\n",
    "        Sources: \n",
    "                https://www.geeksforgeeks.org/ragged-tensors-in-tensorflow/\n",
    "                https://stackoverflow.com/questions/65395179/train-and-predict-on-variable-length-sequences\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert to np.array. Original features are: lat and lon.\n",
    "        for i in range(num_trajectories):\n",
    "            X[i] = np.array(X[i])\n",
    "            Y[i] = np.array(Y[i])\n",
    "\n",
    "        # Convert to ragged tensors\n",
    "        X = tf.ragged.constant(X)\n",
    "        Y = tf.ragged.constant(Y)\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Train Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "def train_data_preparation(X, Y,\n",
    "                            num_of_traj, \n",
    "                            BATCH_SIZE,\n",
    "                            TESTING_SIZE,\n",
    "                            SEQ_LEN,\n",
    "                            NUM_FEATS,\n",
    "                            NUM_OUTPUTS,\n",
    "                            ):\n",
    "    \"\"\"\n",
    "        Train data preparation\n",
    "        Return a numpy array of all trajectories, where each trajectory is padded with zeroes to a multiple of the SL.\n",
    "        After reshaping, each trajectory is a single numpy array of size (NUMBER OF SEQUENCES, SEQ_LEN, NUM_FEATS)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to NP Array and get the sequence lengths\n",
    "    training_size = int(num_of_traj * (1 - TESTING_SIZE))\n",
    "    train_traj_seq_lengths = [0.0] * training_size\n",
    "\n",
    "    X_concatenated, Y_concatenated = [], []\n",
    "\n",
    "    for i in range(training_size):\n",
    "        X[i] = np.array(X[i])\n",
    "        Y[i] = np.array(Y[i])\n",
    "\n",
    "        # For data reshaping later on\n",
    "        reminder = X[i].shape[0] % SEQ_LEN\n",
    "        \n",
    "        # Trim trajectories to seq_len and create a single numpy array with all trajectories\n",
    "        if (X[i].shape[0] >= SEQ_LEN):\n",
    "            X[i] = X[i][0 : (X[i].shape[0] - reminder), :]\n",
    "            Y[i] = Y[i][0 : (Y[i].shape[0] - reminder), :]\n",
    "\n",
    "            X_concatenated.extend(X[i])\n",
    "            Y_concatenated.extend(Y[i])\n",
    "        \n",
    "            train_traj_seq_lengths[i] = X[i].shape[0]\n",
    "            \n",
    "    X_new, Y_new = np.array(X_concatenated), np.array(Y_concatenated)\n",
    "\n",
    "    lstm_data = train_reshape(X_new, Y_new, SEQ_LEN, NUM_FEATS, NUM_OUTPUTS, BATCH_SIZE)\n",
    "\n",
    "    X_train = lstm_data[\"X_train\"]\n",
    "    Y_train = lstm_data[\"Y_train\"]\n",
    "    \n",
    "    return X_train, Y_train, training_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Test Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def test_data_preparation(TRAINING_TESTING_SAME_FILE, \n",
    "                          num_of_traj,\n",
    "                          training_size,\n",
    "                          SEQ_LEN,\n",
    "                          NUM_FEATS,\n",
    "                          TESTING_FILE,\n",
    "                          data,\n",
    "                          X = None, Y = None,\n",
    "                          ):\n",
    "    \n",
    "    \"\"\"\n",
    "        Test data preparation\n",
    "        Returns a list of numpy arrays, where each array is a single trajectory.\n",
    "    \"\"\"\n",
    "\n",
    "    if TRAINING_TESTING_SAME_FILE:\n",
    "        X_test = [0.0] * (num_of_traj - training_size)\n",
    "        Y_test = [0.0] * (num_of_traj - training_size)\n",
    "\n",
    "        test_traj_seq_lengths = [0.0] * (num_of_traj - training_size + 1) \n",
    "\n",
    "        idx = 0\n",
    "        for i in range(training_size, num_of_traj):\n",
    "            X[i] = np.array(X[i])\n",
    "            Y[i] = np.array(Y[i])\n",
    "            \n",
    "            test_traj_seq_lengths[idx] = X[i].shape[0]\n",
    "            \n",
    "            # For data reshaping later on\n",
    "            seq_multiplier = X[i].shape[0] // SEQ_LEN\n",
    "            padding_size = (seq_multiplier + 1) * SEQ_LEN - X[i].shape[0]\n",
    "            \n",
    "            padding = np.zeros([ padding_size, NUM_FEATS ])\n",
    "            \n",
    "            X_test[idx] = np.vstack((X[i], padding))\n",
    "            Y_test[idx] = Y[i]\n",
    "            \n",
    "            idx += 1\n",
    "    else:\n",
    "        # In case the testing data is obtained from a different file:\n",
    "        data_test = load_data_from_pickle(TESTING_FILE)\n",
    "        \n",
    "        X_test = [0.0] * len(data_test)\n",
    "        Y_test = [0.0] * len(data_test)\n",
    "        \n",
    "        # Get trajectories min and max values\n",
    "        num_of_traj_test = len(data_test)\n",
    "\n",
    "        test_traj_seq_lengths = [0.0] * num_of_traj_test\n",
    "        \n",
    "        data_test = [data_test[i][COLUMNS] for i in range(num_of_traj_test)]\n",
    "\n",
    "        scaler, data_test = normalize_trajectory_data(dataset = data_test, normalization_type = 'min-max', testing_data_norm=True, scaler=scaler)\n",
    "\n",
    "        X_t, Y_t = [0.0] * num_of_traj_test, [0.0] * num_of_traj_test\n",
    "\n",
    "        for i in range(num_of_traj_test):\n",
    "            # X is composed of the trajectory data starting from the first point to the second last point\n",
    "            X_t[i] =  data[i][COLUMNS_INPUT].iloc[0:-1] \n",
    "            X_t[i] = X_t[i].fillna(0)\n",
    "            X_t[i].columns = COLUMNS_INPUT\n",
    "\n",
    "            # Y is composed of the trajectory data starting from the second point to the last point\n",
    "            Y_t[i] =  data[i][COLUMNS_OUTPUT].iloc[1:] \n",
    "            Y_t[i] = Y_t[i].fillna(0)\n",
    "            Y_t[i].columns = COLUMNS_OUTPUT\n",
    "\n",
    "        for i in range(num_of_traj_test):\n",
    "            X_t[i] = np.array(X_t[i])\n",
    "            Y_t[i] = np.array(Y_t[i])\n",
    "\n",
    "            test_traj_seq_lengths[i] = X[i].shape[0]\n",
    "            \n",
    "            # For data reshaping later on\n",
    "            seq_multiplier = X_t[i].shape[0] // SEQ_LEN\n",
    "            padding_size = (seq_multiplier + 1) * SEQ_LEN - X_t[i].shape[0]\n",
    "            \n",
    "            padding = np.zeros([ padding_size, NUM_FEATS ])\n",
    "            \n",
    "            X_test[i] = np.vstack((X_t[i], padding))\n",
    "            Y_test[i] = Y_t[i]\n",
    "            \n",
    "    return X_test, Y_test, test_traj_seq_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Distance and Performance Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "        Compute great-circle (Haversine) distance between two lat/lon points in km.\n",
    "        Returns the distance in km.\n",
    "    \"\"\"\n",
    "\n",
    "    # Do this check if one argument is NaN\n",
    "    if pd.isna(lat1) or pd.isna(lon1) or pd.isna(lat2) or pd.isna(lon2):\n",
    "        return 0 # \n",
    "        \n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    return haversine_distances([[lat1, lon1], [lat2, lon2]])[0, 1] * 6371  #Earth radius in km\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance_tdrive(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "        Compute great-circle (Haversine) distance between two lat/lon points in km.\n",
    "        Returns the distance in km.\n",
    "    \"\"\"\n",
    "\n",
    "    # Do this check if one argument is NaN\n",
    "    if pd.isna(lat1) or pd.isna(lon1) or pd.isna(lat2) or pd.isna(lon2):\n",
    "        return np.nan # \n",
    "        \n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    return haversine_distances([[lat1, lon1], [lat2, lon2]])[0, 1] * 6371008.7714  #Earth radius in km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prediction_metrics(Y_test, Y_pred):\n",
    "    \"\"\"\n",
    "        Compute prediction metrics for the model\n",
    "        Returns a dictionary with the following metrics:\n",
    "        - MSE: Mean Squared Error\n",
    "        - KLD: Symetric Kullback-Leibler Divergence (Jeffreys divergence)\n",
    "        - ED: Energy Distance\n",
    "        - WD: Wasserstein Distance\n",
    "    \"\"\"\n",
    "    \n",
    "    # MSE, KLD, ED, WD, HAVERSINE (HS)\n",
    "    mse = mean_squared_error(Y_test, Y_pred)\n",
    "    # kld = keras.losses.KLDivergence(Y_test, Y_pred)\n",
    "    kld = 0\n",
    "    ed = float(energy_distance(Y_test, Y_pred))\n",
    "    wd = float(wasserstein_distance(Y_test, Y_pred))\n",
    "    \n",
    "    results = {\"MSE\" : mse, \"KLD\" : kld, \"ED\" : ed, \"WD\" : wd}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trajectory_metrics(Y_test, Y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "        Compute prediction metrics for the model\n",
    "        Returns a dictionary with the following metrics:\n",
    "        - IE: Individual Error\n",
    "        - ISE: Individual Squared Error\n",
    "        - MSE: Mean Squared Error\n",
    "        - ED: Energy Distance, averaged per features\n",
    "    \"\"\"\n",
    "    \n",
    "    errors = [0.0] * len(Y_test)\n",
    "    squared_errors = [0.0] * len(Y_test)\n",
    "    mses = [0.0] * len(Y_test)\n",
    "    eds = [0.0] * len(Y_test)\n",
    "    \n",
    "    for i in range(len(Y_test)):\n",
    "        \n",
    "        err = Y_test[i] - Y_pred[i]\n",
    "        squared_errors[i] = err**2\n",
    "        errors[i] = err\n",
    "        mses[i] = np.mean(err**2)\n",
    "        \n",
    "        eds1= float(energy_distance(Y_test[i][:,0], Y_pred[i][:,0]))\n",
    "        eds2 = float(energy_distance(Y_test[i][:,1], Y_pred[i][:,1]))\n",
    "        eds[i] = np.mean([eds1, eds2])\n",
    "        \n",
    "    results = {\"IE\" : errors, \"ISE\" : squared_errors, \"MSE\" : mses, \"ED\" : eds}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_point_to_point_haversine_distances(traj1, traj2):\n",
    "    \"\"\"\n",
    "    Compute the haversine point-to-point distance in meters between two trajectories.\n",
    "    \n",
    "    Parameters:\n",
    "        traj1 (array-like): First trajectory as a list or array of [latitude, longitude] pairs.\n",
    "        traj2 (array-like): Second trajectory as a list or array of [latitude, longitude] pairs.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of distances in meters between corresponding points in the two trajectories.\n",
    "    \"\"\"\n",
    "    if len(traj1) != len(traj2):\n",
    "        raise ValueError(\"Trajectories must have the same number of points.\")\n",
    "    \n",
    "    distances = []\n",
    "    for (lat1, lon1), (lat2, lon2) in zip(traj1, traj2):\n",
    "        # Convert degrees to radians\n",
    "        lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "        # Compute haversine distance in kilometers and convert to meters\n",
    "        distance = haversine_distances([[lat1, lon1], [lat2, lon2]])[0, 1] * 6371000  # Earth radius in meters\n",
    "        distances.append(distance)\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_loss(y_true, y_pred):\n",
    "    R = 6371.0  # Earth radius in km\n",
    "    DEG2RAD = math.pi / 180.0\n",
    "\n",
    "    lat1, lon1 = tf.unstack(y_true, axis=-1)\n",
    "    lat2, lon2 = tf.unstack(y_pred, axis=-1)\n",
    "\n",
    "    lat1 = lat1 * DEG2RAD\n",
    "    lon1 = lon1 * DEG2RAD\n",
    "    lat2 = lat2 * DEG2RAD\n",
    "    lon2 = lon2 * DEG2RAD\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = tf.sin(dlat / 2.0)**2 + tf.cos(lat1) * tf.cos(lat2) * tf.sin(dlon / 2.0)**2\n",
    "    c = 2.0 * tf.atan2(tf.sqrt(a), tf.sqrt(1.0 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return tf.reduce_mean(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Trajectory Data Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_act_pred_traj(predicted, \n",
    "                       actual, \n",
    "                       seq_len, \n",
    "                       scatter = False, \n",
    "                       trim_trajectory = False, \n",
    "                       k=1, \n",
    "                       show = True,\n",
    "                       x_range = None,\n",
    "                       y_range = None):\n",
    "    \"\"\"\n",
    "        Plots the actual and predicted trajectories.\n",
    "        Set scatter to True to plot the trajectories as scatter plots.\n",
    "        Set trim_trajectory to True to plot the trajectories with the same length.\n",
    "        Set show to False to not show the plot.\n",
    "        Saves the plot to a file.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not trim_trajectory:\n",
    "        LEN = seq_len\n",
    "        \n",
    "    fig_lons_max = max(actual.lons.max(), predicted.lons.max())\n",
    "    fig_lons_min = min(actual.lons.min(), predicted.lons.min())\n",
    "\n",
    "    fig_lats_max = max(actual.lats.max(), predicted.lats.max())\n",
    "    fig_lats_min = min(actual.lats.min(), predicted.lats.min())\n",
    "    \n",
    "    plt.figure()\n",
    "    if show == True:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.xlim([fig_lons_min, fig_lons_max])\n",
    "    plt.ylim([fig_lats_min, fig_lats_max])\n",
    "    \n",
    "    for act, pred in zip(actual.groupby(\"id\"), predicted.groupby(\"id\")):\n",
    "                   \n",
    "        act_x, act_y = list(act[1].lons), list(act[1].lats) \n",
    "        \n",
    "        if trim_trajectory:\n",
    "            LEN = list(act[1].length)[0]\n",
    "        \n",
    "        if not scatter:\n",
    "            plt.plot(act_x[0:LEN], act_y[0:LEN], marker=\"None\", linestyle=\"-\", linewidth=0.35, color=\"black\")\n",
    "        else:\n",
    "            plt.scatter(act_x[0:LEN], act_y[0:LEN],  linewidth=0.35, color=\"black\")\n",
    "        \n",
    "        pred_x, pred_y = list(pred[1].lons), list(pred[1].lats)\n",
    "        \n",
    "        if not scatter:\n",
    "            plt.plot(pred_x[0:LEN], pred_y[0:LEN], marker=\"None\", linestyle=\"-\", linewidth=0.35, color=\"red\")\n",
    "        else:\n",
    "            plt.scatter(pred_x[0:LEN], pred_y[0:LEN],  linewidth=0.35, color=\"red\", alpha=0.5)\n",
    "        \n",
    "    plt.grid(True)\n",
    "    plt.title(\"Actual vs. Predicted Trajectory for k = \" + str(k))    \n",
    "    plt.legend([\"Actual\", \"Predicted\"])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/Predictions\" + str(k) + \".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_act_pred_traj_one_by_one(predicted, \n",
    "                                    actual, \n",
    "                                    seq_len, \n",
    "                                    scatter = True, \n",
    "                                    trim_trajectory = False, \n",
    "                                    k=1, \n",
    "                                    show = True, \n",
    "                                    num_of_traj_to_plot = 20, \n",
    "                                    start_traj = 0,\n",
    "                                    end_traj = 0,\n",
    "                                    x_range = None,\n",
    "                                    y_range = None,\n",
    "                                    range = None,\n",
    "                                    path = None):\n",
    "    \"\"\"\n",
    "        Plots the actual and predicted trajectories.\n",
    "        Set scatter to True to plot the trajectories as scatter plots.\n",
    "        Set trim_trajectory to True to plot the trajectories with the same length.\n",
    "        Set show to False to not show the plot.\n",
    "        Saves the plot to a file.\n",
    "    \"\"\"\n",
    "           \n",
    "    # fig_lons_max = max(actual.lons.max(), predicted.lons.max())\n",
    "    # fig_lons_min = min(actual.lons.min(), predicted.lons.min())\n",
    "\n",
    "    # fig_lats_max = max(actual.lats.max(), predicted.lats.max())\n",
    "    # fig_lats_min = min(actual.lats.min(), predicted.lats.min())\n",
    "    errors_x = []\n",
    "    errors_y = []\n",
    "    \n",
    "    plt.figure()\n",
    "    if show == True:\n",
    "        plt.show()\n",
    "    \n",
    "    if x_range is not None:\n",
    "        plt.xlim(x_range)\n",
    "        \n",
    "    if y_range is not None:\n",
    "        plt.ylim(y_range)\n",
    "    \n",
    "    # plt.xlim([fig_lons_min, fig_lons_max])\n",
    "    # plt.ylim([fig_lats_min, fig_lats_max])\n",
    "    \n",
    "    index = 0\n",
    "    size = 50\n",
    "    for act, pred in zip(actual, predicted):\n",
    "        \n",
    "        if (end_traj != -1 and index >= start_traj and index <= end_traj) or (range is not None and index in range):\n",
    "                \n",
    "            act_x, act_y = act[:, 0], act[:, 1] \n",
    "            \n",
    "            # Plot the trajectory\n",
    "            if not scatter:\n",
    "                plt.plot(act_x, act_y, marker=\"None\", linestyle=\"-\", linewidth=1, color='tab:blue')\n",
    "            else:\n",
    "                plt.scatter(act_x, act_y,  s = 50, linewidth=1, color='tab:blue')\n",
    "            \n",
    "            # Plot the starting and ending point of the trajectory\n",
    "            # Dont add it to the legend\n",
    "            \n",
    "            # plt.scatter(act_x[0], act_y[0],  s = size, linewidth=0.35, color=\"black\")\n",
    "            # plt.scatter(act_x[-1], act_y[-1],  s = size, linewidth=0.35, color=\"black\", alpha=0.5)\n",
    "            \n",
    "            pred_x, pred_y = pred[:, 0], pred[:, 1]\n",
    "            \n",
    "            if not scatter:\n",
    "                plt.plot(pred_x, pred_y, marker=\"None\", linestyle=\"-\", linewidth=1, color=\"red\")\n",
    "            else:\n",
    "                plt.scatter(pred_x, pred_y, s = 50, linewidth=1, color=\"red\", alpha=0.5)\n",
    "            \n",
    "            # Plot the starting and ending point of the trajectory\n",
    "            # plt.scatter(pred_x[0], pred_y[0],  s = size, linewidth=0.35, color=\"red\")\n",
    "            # plt.scatter(pred_x[-1], pred_y[-1],  s = size, linewidth=0.35, color=\"red\", alpha=0.5)\n",
    "            \n",
    "            errors_x.append(act_x - pred_x)\n",
    "            errors_y.append(act_y - pred_y)\n",
    "            \n",
    "        index += 1\n",
    "        \n",
    "    plt.grid(True)\n",
    "    plt.title(\"Actual vs. Predicted Trajectory\")    \n",
    "    plt.legend([\"Actual\", \"Predicted\" ])\n",
    "    # plt.show()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if path is not None:\n",
    "        plt.savefig(path + \"Predictions\" + str(k) + \".pdf\")\n",
    "    else:\n",
    "        plt.savefig(\"plots/Predictions\" + str(k) + \".pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Create LSTM and Recurrent Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_model(LSTM_cells, \n",
    "                      seq_len, \n",
    "                      num_feat,\n",
    "                      batch_size,\n",
    "                      stateful,\n",
    "                      return_seq,\n",
    "                      num_outputs,\n",
    "                      LR,\n",
    "                      SEED,\n",
    "                      ragged = False):\n",
    "    \"\"\"\n",
    "        Create an LSTM model with the specified parameters\n",
    "        Returns the untrained model\n",
    "    \"\"\"\n",
    "    \n",
    "    keras.utils.set_random_seed(SEED)\n",
    "\n",
    "    # In newer versions of Keras, for stateful LSTM, you need to specify the batch_input_shape as the first layer (input layer)\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Ragged tensor for variable length sequences\n",
    "    if ragged is False:\n",
    "        model.add(keras.layers.InputLayer(batch_input_shape=(batch_size, seq_len, num_feat)))\n",
    "        model.add(keras.layers.LSTM(LSTM_cells, return_sequences = return_seq, stateful = stateful))\n",
    "    else:\n",
    "        model.add(keras.layers.InputLayer(shape=[None, num_feat], batch_size = batch_size, dtype=tf.float32, ragged = True))\n",
    "        model.add(keras.layers.LSTM(LSTM_cells, return_sequences = return_seq, stateful = False))\n",
    "        \n",
    "    model.add(keras.layers.Dense(num_outputs))\n",
    "    \n",
    "    \n",
    "    # https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate = LR,\n",
    "        decay_steps = 50,\n",
    "        decay_rate = 0.99) \n",
    "    \n",
    "    model.compile(optimizer = \"adam\", \n",
    "                  loss = \"mse\", \n",
    "                  #loss = haversine_loss,\n",
    "                  #metrics = [\"mse\", \"mae\", \"mape\", \"kl_divergence\"])\n",
    "                  metrics = [\"mse\"])\n",
    "    \n",
    "    # https://keras.io/api/optimizers/\n",
    "    model.optimizer.lr=lr_schedule\n",
    "    # mdl.optimizer.momentum = 0.99\n",
    "    # mdl.optimizer.use_ema = True\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_model(RNN_cells, \n",
    "                      seq_len, \n",
    "                      num_feat,\n",
    "                      batch_size,\n",
    "                      stateful,\n",
    "                      return_seq,\n",
    "                      num_outputs,\n",
    "                      LR,\n",
    "                      SEED,\n",
    "                      ragged = False):\n",
    "    \"\"\"\n",
    "        Create an LSTM model with the specified parameters\n",
    "        Returns the untrained model\n",
    "    \"\"\"\n",
    "    \n",
    "    keras.utils.set_random_seed(SEED)\n",
    "\n",
    "    # In newer versions of Keras, for stateful LSTM, you need to specify the batch_input_shape as the first layer (input layer)\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Ragged tensor for variable length sequences\n",
    "    if ragged is False:\n",
    "        model.add(keras.layers.InputLayer(batch_input_shape=(batch_size, seq_len, num_feat)))\n",
    "        model.add(keras.layers.SimpleRNN(RNN_cells, return_sequences = return_seq, stateful = stateful))\n",
    "    else:\n",
    "        model.add(keras.layers.InputLayer(shape=[None, num_feat], batch_size = batch_size, dtype=tf.float32, ragged = True))\n",
    "        model.add(keras.layers.SimpleRNN(RNN_cells, return_sequences = return_seq, stateful = False))\n",
    "        \n",
    "    model.add(keras.layers.Dense(num_outputs))\n",
    "    \n",
    "    \n",
    "    # https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate = LR,\n",
    "        decay_steps = 40,\n",
    "        decay_rate = 0.96) \n",
    "    \n",
    "    model.compile(optimizer = \"adam\", \n",
    "                  loss = \"mse\", \n",
    "                  metrics = [\"mse\", \"mae\", \"mape\", \"kl_divergence\"])\n",
    "    \n",
    "    # https://keras.io/api/optimizers/\n",
    "    model.optimizer.lr=lr_schedule\n",
    "    # mdl.optimizer.momentum = 0.99\n",
    "    # mdl.optimizer.use_ema = True\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_GRU_model(GRU_cells, \n",
    "                      seq_len, \n",
    "                      num_feat,\n",
    "                      batch_size,\n",
    "                      stateful,\n",
    "                      return_seq,\n",
    "                      num_outputs,\n",
    "                      LR,\n",
    "                      SEED,\n",
    "                      ragged = False):\n",
    "    \"\"\"\n",
    "        Create an GRU model with the specified parameters\n",
    "        Returns the untrained model\n",
    "    \"\"\"\n",
    "    \n",
    "    keras.utils.set_random_seed(SEED)\n",
    "\n",
    "    # In newer versions of Keras, for stateful LSTM, you need to specify the batch_input_shape as the first layer (input layer)\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Ragged tensor for variable length sequences\n",
    "    if ragged is False:\n",
    "        model.add(keras.layers.InputLayer(batch_input_shape=(batch_size, seq_len, num_feat)))\n",
    "        model.add(keras.layers.GRU(GRU_cells, return_sequences = return_seq, stateful = stateful))\n",
    "    else:\n",
    "        model.add(keras.layers.InputLayer(shape=[None, num_feat], batch_size = batch_size, dtype=tf.float32, ragged = True))\n",
    "        model.add(keras.layers.GRU(GRU_cells, return_sequences = return_seq, stateful = False))\n",
    "        \n",
    "    model.add(keras.layers.Dense(num_outputs))\n",
    "    \n",
    "    \n",
    "    # https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate = LR,\n",
    "        decay_steps = 40,\n",
    "        decay_rate = 0.96) \n",
    "    \n",
    "    model.compile(optimizer = \"adam\", \n",
    "                  loss = \"mse\", \n",
    "                  metrics = [\"mse\", \"mae\", \"mape\", \"kl_divergence\"])\n",
    "    \n",
    "    # https://keras.io/api/optimizers/\n",
    "    model.optimizer.lr=lr_schedule\n",
    "    # mdl.optimizer.momentum = 0.99\n",
    "    # mdl.optimizer.use_ema = True\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Train LSTM Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras training loop\n",
    "def train_model(model, \n",
    "                X_train, Y_train, \n",
    "                epochs,\n",
    "                batch_size):\n",
    "    \"\"\"\n",
    "        Train the LSTM model with the default keras method\n",
    "        Returns the training history and the trained model\n",
    "    \"\"\"\n",
    "    history = model.fit(X_train, \n",
    "                        Y_train, \n",
    "                        batch_size = batch_size, \n",
    "                        epochs = epochs, \n",
    "                        verbose = 0, \n",
    "                        shuffle = False)\n",
    "        \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual training loop\n",
    "def train_model_manual_loop(model, \n",
    "                            epochs,\n",
    "                            LR,\n",
    "                            batch_size,\n",
    "                            X_train = None,\n",
    "                            Y_train = None,\n",
    "                            reset_states = True):\n",
    "    \"\"\"\n",
    "        Manual training loop for the LSTM model\n",
    "        Works with the model created with the create_LSTM_model function\n",
    "        Works for any batch size\n",
    "        Returns the training history and the trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_fn = keras.losses.MeanSquaredError()\n",
    "    \n",
    "    optimizer = model.optimizer\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        for step in range(0, len(X_train), batch_size):\n",
    "\n",
    "            x_batch = X_train[step : step + batch_size]  \n",
    "            y_batch = Y_train[step : step + batch_size]  \n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(x_batch, training=True)\n",
    "                loss = loss_fn(y_batch, predictions)\n",
    "\n",
    "            # Get gradients\n",
    "            trainable_vars = model.trainable_variables\n",
    "            gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "            # Apply gradients to update model weights\n",
    "            optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "            \n",
    "            # Reset states after batch\n",
    "            if reset_states:\n",
    "                model.reset_states()\n",
    "            \n",
    "    return model.history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual training loop with sequence resets\n",
    "def train_model_train_on_batch(model, \n",
    "                                X, Y, \n",
    "                                batch_size , \n",
    "                                epochs):\n",
    "    \"\"\"\n",
    "        Manual training loop with sequence resets and train on batch\n",
    "        Works for stateful and stateless models\n",
    "        Works for any batch size\n",
    "        Returns the trained model\n",
    "    \"\"\"\n",
    "    # Manual training loop with sequence resets used with train_on_baatch\n",
    "    for epoch in range(epochs):\n",
    "        for start in range(0, len(X), batch_size):\n",
    "            model.train_on_batch(X[start : start + batch_size], Y[start : start + batch_size])\n",
    "            model.reset_states()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Test LSTM Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, \n",
    "               X_test, \n",
    "               Y_test, \n",
    "               batch_size):\n",
    "    \"\"\"\n",
    "        Works for both stateful and non-stateful models\n",
    "        Works for any batch size\n",
    "        Returns the predictions of the model on the test data\n",
    "    \"\"\"\n",
    "    \n",
    "    Y_pred = model.predict(X_test, batch_size = batch_size, verbose = 0).reshape(-1, 1).flatten()\n",
    "    Y_test = Y_test.reshape(-1, 1).flatten()\n",
    "    \n",
    "    return Y_test, Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_per_trajectory(mdl, \n",
    "                            X_t, \n",
    "                            test_traj_seq_lengths,\n",
    "                            SEQ_LENGTH,\n",
    "                            NUM_FEATS\n",
    "                            ):\n",
    "    \"\"\"\n",
    "        Works for both stateful and non-stateful models\n",
    "        Works for any batch size\n",
    "        Returns the predictions of the model on the test data\n",
    "    \"\"\"\n",
    "    mdl.layers[0].reset_states()\n",
    "    \n",
    "    Y_preds = [0.0] * len(X_t)\n",
    "\n",
    "    for i in range(len(X_t)):\n",
    "        X_t[i] = X_t[i].reshape(-1, SEQ_LENGTH, NUM_FEATS)\n",
    "    \n",
    "    for i in range(len(X_t)):\n",
    "        \n",
    "        y_pred = mdl.predict(X_t[i], batch_size = 1, verbose = 0).reshape(-1, NUM_OUTPUTS)\n",
    "        y_pred = y_pred[0:test_traj_seq_lengths[i], :]\n",
    "        Y_preds[i] = y_pred\n",
    "        \n",
    "        mdl.layers[0].reset_states()\n",
    "        \n",
    "    return Y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_all_trajectories(model, \n",
    "                                X_test, \n",
    "                                Y_test, \n",
    "                                batch_size):\n",
    "    \"\"\"\n",
    "        Works for both stateful and non-stateful models\n",
    "        Works for any batch size\n",
    "        Returns the predictions of the model on the test data\n",
    "    \"\"\"\n",
    "    \n",
    "    Y_pred = model.predict(X_test, batch_size = batch_size).reshape(-1, 1).flatten()\n",
    "    Y_test = Y_test.reshape(-1, 1).flatten()\n",
    "   \n",
    "    return Y_test, Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Test with Dummy Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load Trajectory Data Once**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trajectory data for faster execution\n",
    "data = load_data_from_pickle(DATASET[SELECTED_DATASET], TOTAL_TRAJS)\n",
    "\n",
    "# Get the data in the selected square\n",
    "data = get_data_in_square(data = data, square = DATA_SQUARE[SELECTED_DATASET])\n",
    "\n",
    "# Get trajectories min and max values\n",
    "mins, maxs =  get_min_max_from_data(data)\n",
    "\n",
    "# Get number of trajectories\n",
    "num_of_traj = len(data)\n",
    "\n",
    "# Normalize the data using the min and max values\n",
    "normalization_ranges = {\"min\": mins, \"max\": maxs}\n",
    "\n",
    "# Only keep the lat and lon columns for now\n",
    "data = [data[i][COLUMNS] for i in range(num_of_traj)]\n",
    "\n",
    "# Normalize the data using scaler or normalization ranges\n",
    "scaler, data = normalize_trajectory_data(dataset = data, normalization_type = 'min-max')\n",
    "\n",
    "# Create X and Y from the data\n",
    "X, Y =  create_X_Y_from_data(data, num_of_traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create Trajectory Data Training/Testing Splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data Preparation\n",
    "X_train, Y_train, training_size = train_data_preparation(X = copy.deepcopy(X) , Y= copy.deepcopy(Y),\n",
    "                                                        num_of_traj = num_of_traj,\n",
    "                                                        BATCH_SIZE = BATCH_SIZE,\n",
    "                                                        TESTING_SIZE = TESTING_SIZE,\n",
    "                                                        SEQ_LEN = SEQ_LEN,\n",
    "                                                        NUM_FEATS = NUM_FEATS,\n",
    "                                                        NUM_OUTPUTS = NUM_OUTPUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data Preparation\n",
    "X_test, Y_test, test_traj_seq_lengths = test_data_preparation(TRAINING_TESTING_SAME_FILE = TRAINING_TESTING_SAME_FILE,\n",
    "                                                                X = copy.deepcopy(X), Y = copy.deepcopy(Y),\n",
    "                                                                num_of_traj = num_of_traj,\n",
    "                                                                training_size = training_size,\n",
    "                                                                SEQ_LEN = SEQ_LEN,\n",
    "                                                                NUM_FEATS = NUM_FEATS,\n",
    "                                                                TESTING_FILE = None,\n",
    "                                                                data = data)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CPU and RAM measurments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749714533.653074 2144945 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1059 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Model TRAINING:\n",
    "\n",
    "mdl1 = create_LSTM_model(LSTM_cells = LSTM_CELLS,\n",
    "                          seq_len = SEQ_LEN,\n",
    "                          num_feat = NUM_FEATS,\n",
    "                          batch_size = BATCH_SIZE,\n",
    "                          stateful = STATEFUL,\n",
    "                          return_seq = RETURN_SEQ,\n",
    "                          num_outputs = NUM_OUTPUTS,\n",
    "                          LR = LR,\n",
    "                          SEED = SEED,\n",
    "                          ragged = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<unknown>, line 3)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/roland/traj_exp/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3457\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"/tmp/ipykernel_2144945/1019547037.py\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    get_ipython().run_cell_magic('time', '', '# Train the model\\n%memit history, mdl1 = train_model(model = mdl1,\\n                    X_train = X_train,\\n                    Y_train = Y_train,\\n                    epochs = EPOCHS,\\n                    batch_size = BATCH_SIZE)\\n')\n",
      "  File \u001b[1;32m\"/home/roland/traj_exp/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2419\u001b[0m, in \u001b[1;35mrun_cell_magic\u001b[0m\n    result = fn(*args, **kwargs)\n",
      "  File \u001b[1;32m\"/home/roland/traj_exp/venv/lib/python3.10/site-packages/decorator.py\"\u001b[0m, line \u001b[1;32m232\u001b[0m, in \u001b[1;35mfun\u001b[0m\n    return caller(func, *(extras + args), **kw)\n",
      "  File \u001b[1;32m\"/home/roland/traj_exp/venv/lib/python3.10/site-packages/IPython/core/magic.py\"\u001b[0m, line \u001b[1;32m187\u001b[0m, in \u001b[1;35m<lambda>\u001b[0m\n    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \u001b[1;32m\"/home/roland/traj_exp/venv/lib/python3.10/site-packages/IPython/core/magics/execution.py\"\u001b[0m, line \u001b[1;32m1291\u001b[0m, in \u001b[1;35mtime\u001b[0m\n    expr_ast = self.shell.compile.ast_parse(expr)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/roland/traj_exp/venv/lib/python3.10/site-packages/IPython/core/compilerop.py\"\u001b[0;36m, line \u001b[0;32m101\u001b[0;36m, in \u001b[0;35mast_parse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    X_train = X_train,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from memory_profiler import profile\n",
    "\n",
    "%%time\n",
    "@profile\n",
    "# Train the model\n",
    "history, mdl1 = train_model(model = mdl1,\n",
    "                    X_train = X_train,\n",
    "                    Y_train = Y_train,\n",
    "                    epochs = EPOCHS,\n",
    "                    batch_size = BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Point-to-Point MODEL TESTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model TESTING Point-to-Point:\n",
    "\n",
    "\"\"\"\n",
    "    Test the model with BS = 1 ans SL1\n",
    "    \n",
    "    Returns:\n",
    "        Y_Test and Y_pred: List of numpy arrays of length of the original trajectories without zero padding.\n",
    "\"\"\"\n",
    "\n",
    "model_sl2 = create_LSTM_model(LSTM_cells = LSTM_CELLS,\n",
    "                          seq_len = 1,\n",
    "                          num_feat = NUM_FEATS,\n",
    "                          batch_size = 1,\n",
    "                          stateful = True,\n",
    "                          return_seq = RETURN_SEQ,\n",
    "                          num_outputs = NUM_OUTPUTS,\n",
    "                          LR = LR,\n",
    "                          SEED = SEED,\n",
    "                          ragged = False)\n",
    "\n",
    "# Set weights and states\n",
    "model_sl2.set_weights(mdl1.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%memit\n",
    "# Test the model with a point-to-point approach approach\n",
    "Y_pred_2 = test_model_per_trajectory(mdl = model_sl2,\n",
    "                                            X_t = X_test,\n",
    "                                            test_traj_seq_lengths = test_traj_seq_lengths,\n",
    "                                            SEQ_LENGTH = 1,\n",
    "                                            NUM_FEATS = NUM_FEATS)\n",
    "\n",
    "actual = denormalize_data(dataset = Y_test, scaler = scaler)\n",
    "predicted = denormalize_data(dataset = Y_pred_2, scaler = scaler)\n",
    "\n",
    "for act, pred in zip(actual, predicted):\n",
    "    res = compute_point_to_point_haversine_distances(act, pred )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
